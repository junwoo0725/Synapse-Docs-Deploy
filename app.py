import os
from io import BytesIO

import numpy as np
import streamlit as st
from dotenv import load_dotenv
from pypdf import PdfReader
from pdf2image import convert_from_bytes
import pytesseract

pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

from openai import OpenAI
import ollama  # ë¡œì»¬ LLMìš©


# ==========================
# 0) í™˜ê²½ ë³€ìˆ˜ ë¡œë”© & LLM ì œê³µì ê²°ì •
# ==========================
load_dotenv()

ENV_PROVIDER = os.getenv("LLM_PROVIDER", "openai").lower()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")  # ì—†ì„ ìˆ˜ë„ ìˆìŒ
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "gemma3")
OLLAMA_EMBED_MODEL = os.getenv("OLLAMA_EMBED_MODEL", "gemma3")

# OPENAI ì„¤ì •ì¸ë° í‚¤ê°€ ì—†ìœ¼ë©´ â†’ ìë™ìœ¼ë¡œ ollamaë¡œ ë³€ê²½
if ENV_PROVIDER == "openai" and not OPENAI_API_KEY:
    EFFECTIVE_PROVIDER = "ollama"
    FALLBACK_REASON = "no_openai_key"
else:
    EFFECTIVE_PROVIDER = ENV_PROVIDER
    FALLBACK_REASON = None


class LLMClient:
    """OpenAI / Ollamaë¥¼ í•˜ë‚˜ì˜ ì¸í„°í˜ì´ìŠ¤ë¡œ ë¬¶ëŠ” í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self, provider: str):
        self.provider = provider
        if provider == "openai":
            if not OPENAI_API_KEY:
                # ì•ˆì „ì¥ì¹˜
                raise ValueError(
                    "OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. .envì— í‚¤ë¥¼ ë„£ê±°ë‚˜ LLM_PROVIDER=ollamaë¡œ ë³€ê²½í•˜ì„¸ìš”."
                )
            self.client = OpenAI()
        elif provider == "ollama":
            self.client = None  # ollamaëŠ” ì „ì—­ í•¨ìˆ˜ë¡œ í˜¸ì¶œ
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM provider: {provider}")

    # --------- í…ìŠ¤íŠ¸ ìš”ì•½ ----------
    def summarize(self, text: str, mode: str = "overview", level: str = "ê¸°ë³¸") -> str:
        """
        mode:
          - overview: ì „ì²´ ê°œìš” ìš”ì•½
          - keywords: í‚¤ì›Œë“œ/íƒœê·¸ë§Œ ë½‘ê¸°
          - toc: ë¬¸ì„œ êµ¬ì¡°/ëª©ì°¨ ìƒì„±
          - highlights: ì¤‘ìš”í•œ ë¬¸ì¥/í¬ì¸íŠ¸ ì¶”ì¶œ
        level:
          - ì§§ê²Œ / ê¸°ë³¸ / ìì„¸íˆ (overview ëª¨ë“œì—ì„œë§Œ ì‚¬ìš©)
        """
        # -------- OpenAI ê¸°ë°˜ ìš”ì•½ --------
        if self.provider == "openai":
            if mode == "overview":
                system_prompt = (
                    "ë‹¹ì‹ ì€ í•œêµ­ì–´ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ ë¹„ì„œì…ë‹ˆë‹¤. "
                    "ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ëŠ” ì¤„ì´ê³  í•µì‹¬ ì•„ì´ë””ì–´ ìœ„ì£¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”."
                )
                if level == "ì§§ê²Œ":
                    length_instruction = "í•œë‘ ë¬¸ì¥ìœ¼ë¡œ ì•„ì£¼ ì§§ê²Œ ìš”ì•½í•´ì¤˜."
                elif level == "ìì„¸íˆ":
                    length_instruction = (
                        "í•µì‹¬ ì£¼ì œ, ì£¼ìš” ì£¼ì¥, ê·¼ê±°, ê²°ë¡ ì„ í¬í•¨í•´ì„œ 15~20ì¤„ ì •ë„ë¡œ ìì„¸íˆ ìš”ì•½í•´ì¤˜."
                    )
                else:  # ê¸°ë³¸
                    length_instruction = (
                        "í•µì‹¬ ì£¼ì œ, ì£¼ìš” ì£¼ì¥, ì¤‘ìš”í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ í¬í•¨í•´ì„œ 7~10ì¤„ ì •ë„ë¡œ ìš”ì•½í•´ì¤˜."
                    )
                user_prompt = (
                    f"{length_instruction}\n\n"
                    f"ë‹¤ìŒì€ ìš”ì•½í•  ë¬¸ì„œ ì „ì²´ ë‚´ìš©ì´ë‹¤:\n\n{text}"
                )
            elif mode == "keywords":
                system_prompt = (
                    "ë‹¹ì‹ ì€ í•œêµ­ì–´ ë¬¸ì„œì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ë½‘ì•„ë‚´ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
                    "ì¤‘ë³µë˜ê±°ë‚˜ ì˜ë¯¸ê°€ ì•½í•œ ë‹¨ì–´ëŠ” ì œì™¸í•˜ì„¸ìš”."
                )
                user_prompt = (
                    "ë‹¤ìŒ ë¬¸ì„œì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ/íƒœê·¸ë¥¼ 5~10ê°œ ì •ë„ ë½‘ì•„ì¤˜. "
                    "ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì„œ í•œ ì¤„ë¡œë§Œ ì¶œë ¥í•´ì¤˜.\n\n"
                    f"{text}"
                )
            elif mode == "toc":
                system_prompt = (
                    "ë‹¹ì‹ ì€ í•œêµ­ì–´ ê¸°ìˆ  ë¬¸ì„œì˜ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ë…¼ë¦¬ì ì¸ ëª©ì°¨ë¥¼ ë§Œë“œëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
                    "ì‹¤ì œ ë¬¸ë‹¨ êµ¬ì¡°ë¥¼ ì¶”ë¡ í•´ ìƒìœ„/í•˜ìœ„ ì„¹ì…˜ì„ ì •ë¦¬í•˜ì„¸ìš”."
                )
                user_prompt = (
                    "ë‹¤ìŒ ë¬¸ì„œë¥¼ ì½ê³ , ì‹¤ì œ ë‚´ìš© ìˆœì„œì— ë§ëŠ” ëª©ì°¨ë¥¼ ë§Œë“¤ì–´ì¤˜.\n"
                    "- ìˆ«ì ëª©ë¡ í˜•íƒœë¡œ ì¶œë ¥í•´ì¤˜ (ì˜ˆ: 1. ê°œìš”, 2. ë°°ê²½, 3. ê²°ë¡ ).\n"
                    "- ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì„ ìƒˆë¡œ ë§Œë“¤ì§€ ë§ê³ , ì‹¤ì œë¡œ ë“±ì¥í•˜ëŠ” ì£¼ì œë§Œ ì‚¬ìš©í•´.\n\n"
                    f"{text}"
                )
            elif mode == "highlights":
                system_prompt = (
                    "ë‹¹ì‹ ì€ ê¸´ ë¬¸ì„œì—ì„œ ì¤‘ìš”í•œ ë¬¸ì¥ë§Œ ê³¨ë¼ í•˜ì´ë¼ì´íŠ¸í•´ì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
                    "í•µì‹¬ ì£¼ì¥, ì¤‘ìš”í•œ ì •ì˜, ê²°ë¡ ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì¥ì„ ìš°ì„ ì ìœ¼ë¡œ ì„ íƒí•˜ì„¸ìš”."
                )
                user_prompt = (
                    "ë‹¤ìŒ ë¬¸ì„œì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë¬¸ì¥ 5~10ê°œë¥¼ ê³¨ë¼ì¤˜.\n"
                    "- ê° ë¬¸ì¥ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°œì·Œí•´.\n"
                    "- ìƒˆë¡œìš´ ë¬¸ì¥ì„ ë§Œë“¤ì§€ ë§ê³ , ë¬¸ì„œì— ì‹¤ì œë¡œ ìˆëŠ” ë¬¸ì¥ë§Œ ì‚¬ìš©í•´.\n"
                    "- ê° ë¬¸ì¥ ì•ì—ëŠ” '- 'ë¥¼ ë¶™ì—¬ bullet ëª©ë¡ìœ¼ë¡œ ì¶œë ¥í•´ì¤˜.\n\n"
                    f"{text}"
                )
            else:
                # ì•Œë ¤ì§€ì§€ ì•Šì€ ëª¨ë“œëŠ” ê¸°ë³¸ ìš”ì•½ìœ¼ë¡œ ì²˜ë¦¬
                system_prompt = (
                    "ë‹¹ì‹ ì€ í•œêµ­ì–´ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ ë¹„ì„œì…ë‹ˆë‹¤. "
                    "í•µì‹¬ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ ì£¼ì„¸ìš”."
                )
                user_prompt = f"ë‹¤ìŒ ë¬¸ì„œë¥¼ ìš”ì•½í•´ì¤˜:\n\n{text}"

            resp = self.client.responses.create(
                model="gpt-4.1-mini",
                input=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
            )
            return resp.output_text

        # -------- Ollama ê¸°ë°˜ ìš”ì•½ --------
        elif self.provider == "ollama":
            if mode == "overview":
                system_prompt = (
                    "ë‹¹ì‹ ì€ í•œêµ­ì–´ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ëŠ” ì˜¤í”„ë¼ì¸ ë¹„ì„œì…ë‹ˆë‹¤. "
                    "ìˆ˜ì¤€ ë†’ê²Œ ìš”ì•½í•˜ë˜, ë¶ˆí•„ìš”í•œ ë§ì€ ì¤„ì´ê³  í•µì‹¬ë§Œ ì •ë¦¬í•˜ì„¸ìš”."
                )
                if level == "ì§§ê²Œ":
                    length_instruction = "í•œë‘ ë¬¸ì¥ìœ¼ë¡œ ì•„ì£¼ ì§§ê²Œ ìš”ì•½í•´ì¤˜."
                elif level == "ìì„¸íˆ":
                    length_instruction = (
                        "í•µì‹¬ ì£¼ì œ, ì£¼ìš” ì£¼ì¥, ê·¼ê±°, ê²°ë¡ ì„ í¬í•¨í•´ì„œ 15~20ì¤„ ì •ë„ë¡œ ìì„¸íˆ ìš”ì•½í•´ì¤˜."
                    )
                else:  # ê¸°ë³¸
                    length_instruction = (
                        "í•µì‹¬ ì£¼ì œ, ì£¼ìš” ì£¼ì¥, ì¤‘ìš”í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ í¬í•¨í•´ì„œ 7~10ì¤„ ì •ë„ë¡œ ìš”ì•½í•´ì¤˜."
                    )
                user_content = (
                    f"{length_instruction}\n\n"
                    f"ë‹¤ìŒì€ ìš”ì•½í•  ë¬¸ì„œ ì „ì²´ ë‚´ìš©ì´ë‹¤:\n\n{text}"
                )
            elif mode == "keywords":
                system_prompt = (
                    "ë‹¹ì‹ ì€ í•œêµ­ì–´ ë¬¸ì„œì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œë§Œ ì¶”ë ¤ë‚´ëŠ” ì˜¤í”„ë¼ì¸ ë¹„ì„œì…ë‹ˆë‹¤."
                )
                user_content = (
                    "ë‹¤ìŒ ë¬¸ì„œì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ/íƒœê·¸ë¥¼ 5~10ê°œ ì •ë„ ë½‘ì•„ì¤˜. "
                    "ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì„œ í•œ ì¤„ë¡œë§Œ ì¶œë ¥í•´ì¤˜.\n\n"
                    f"{text}"
                )
            elif mode == "toc":
                system_prompt = (
                    "ë‹¹ì‹ ì€ í•œêµ­ì–´ ë¬¸ì„œë¥¼ ì½ê³  êµ¬ì¡°ë¥¼ íŒŒì•…í•´ ëª©ì°¨ë¥¼ ë§Œë“¤ì–´ì£¼ëŠ” ì˜¤í”„ë¼ì¸ ë¹„ì„œì…ë‹ˆë‹¤. "
                    "ë¬¸ì„œì˜ íë¦„ì„ ë³´ê³  ìƒìœ„/í•˜ìœ„ ì„¹ì…˜ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë‚˜ëˆ„ì„¸ìš”."
                )
                user_content = (
                    "ë‹¤ìŒ ë¬¸ì„œë¥¼ ì½ê³ , ì‹¤ì œ ë‚´ìš© ìˆœì„œì— ë§ëŠ” ëª©ì°¨ë¥¼ ë§Œë“¤ì–´ì¤˜.\n"
                    "- ìˆ«ì ëª©ë¡ í˜•íƒœë¡œ ì¶œë ¥í•´ì¤˜ (ì˜ˆ: 1. ê°œìš”, 2. ë°°ê²½, 3. ê²°ë¡ ).\n"
                    "- ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì„ ìƒˆë¡œ ë§Œë“¤ì§€ ë§ê³ , ì‹¤ì œë¡œ ë“±ì¥í•˜ëŠ” ì£¼ì œë§Œ ì‚¬ìš©í•´.\n\n"
                    f"{text}"
                )
            elif mode == "highlights":
                system_prompt = (
                    "ë‹¹ì‹ ì€ ê¸´ í•œêµ­ì–´ ë¬¸ì„œì—ì„œ ì¤‘ìš” ë¬¸ì¥ë§Œ ê³¨ë¼ì£¼ëŠ” ì˜¤í”„ë¼ì¸ ë¹„ì„œì…ë‹ˆë‹¤. "
                    "í•µì‹¬ ì£¼ì¥, ì •ì˜, ê²°ë¡ ì„ ëŒ€í‘œí•˜ëŠ” ë¬¸ì¥ì„ ì„ íƒí•˜ì„¸ìš”."
                )
                user_content = (
                    "ë‹¤ìŒ ë¬¸ì„œì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë¬¸ì¥ 5~10ê°œë¥¼ ê³¨ë¼ì¤˜.\n"
                    "- ê° ë¬¸ì¥ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë°œì·Œí•´.\n"
                    "- ìƒˆë¡œìš´ ë¬¸ì¥ì„ ë§Œë“¤ì§€ ë§ê³ , ë¬¸ì„œì— ì‹¤ì œë¡œ ìˆëŠ” ë¬¸ì¥ë§Œ ì‚¬ìš©í•´.\n"
                    "- ê° ë¬¸ì¥ ì•ì—ëŠ” '- 'ë¥¼ ë¶™ì—¬ bullet ëª©ë¡ìœ¼ë¡œ ì¶œë ¥í•´ì¤˜.\n\n"
                    f"{text}"
                )
            else:
                system_prompt = (
                    "ë‹¹ì‹ ì€ í•œêµ­ì–´ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ëŠ” ì˜¤í”„ë¼ì¸ ë¹„ì„œì…ë‹ˆë‹¤. "
                    "í•µì‹¬ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ ì£¼ì„¸ìš”."
                )
                user_content = f"ë‹¤ìŒ ë¬¸ì„œë¥¼ ìš”ì•½í•´ì¤˜:\n\n{text}"

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content},
            ]
            response = ollama.chat(model=OLLAMA_MODEL, messages=messages)
            # ollama-pythonì—ì„œ messageëŠ” dictì¼ ìˆ˜ë„ ìˆê³  ê°ì²´ì¼ ìˆ˜ë„ ìˆì–´ì„œ ë‘˜ ë‹¤ ì²˜ë¦¬
            if hasattr(response, "message"):
                msg = response.message
                if isinstance(msg, dict):
                    return msg.get("content", "")
                return msg.content
            return ""

        else:
            raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” provider")

    # --------- ì„ë² ë”© ----------
    def embed(self, texts: list[str]) -> np.ndarray:
        """ì—¬ëŸ¬ ë¬¸ì¥ì— ëŒ€í•´ ì„ë² ë”© ë²¡í„° ìƒì„±"""
        if self.provider == "openai":
            emb = self.client.embeddings.create(
                model="text-embedding-3-small",
                input=texts,
            )
            vectors = [d.embedding for d in emb.data]
            return np.array(vectors, dtype="float32")

        elif self.provider == "ollama":
            resp = ollama.embed(model=OLLAMA_EMBED_MODEL, input=texts)
            return np.array(resp["embeddings"], dtype="float32")

        else:
            raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” provider")


# ì „ì—­ LLM í´ë¼ì´ì–¸íŠ¸ í•˜ë‚˜ë§Œ ìƒì„±
llm = LLMClient(EFFECTIVE_PROVIDER)


# ==========================
# 2) ìœ í‹¸ í•¨ìˆ˜ë“¤
# ==========================
def extract_text_from_file(uploaded_file) -> str:
    """PDF / TXTì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì´ë¯¸ì§€ OCR í¬í•¨)"""
    name_lower = uploaded_file.name.lower()

    # PDF
    if uploaded_file.type == "application/pdf" or name_lower.endswith(".pdf"):
        data = uploaded_file.read()
        reader = PdfReader(BytesIO(data))
        texts = []

        for page_num, page in enumerate(reader.pages):
            text = page.extract_text()
            if text and len(text.strip()) > 10:
                # í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                texts.append(text)
            else:
                # í…ìŠ¤íŠ¸ê°€ ê±°ì˜ ì—†ìœ¼ë©´ OCR ì‹œë„
                st.write(f"ğŸ“· í˜ì´ì§€ {page_num+1} OCR ë¶„ì„ ì¤‘...")
                images = convert_from_bytes(
                    data,
                    first_page=page_num + 1,
                    last_page=page_num + 1,
                    poppler_path=r"C:\poppler\Library\bin",
                )
                ocr_text = ""
                for img in images:
                    ocr_text += pytesseract.image_to_string(img, lang="kor+eng")
                texts.append(ocr_text)

        return "\n".join(texts)

    # TXT
    if uploaded_file.type.startswith("text/") or name_lower.endswith(".txt"):
        data = uploaded_file.read()
        try:
            return data.decode("utf-8")
        except UnicodeDecodeError:
            return data.decode("cp949", errors="ignore")

    return ""


def chunk_text(text: str, max_chars: int = 1200) -> list[str]:
    """ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì¼ì • ê¸¸ì´ë¡œ ë‚˜ëˆ„ê¸°"""
    chunks = []
    start = 0
    length = len(text)
    while start < length:
        end = start + max_chars
        chunks.append(text[start:end])
        start = end
    return chunks


def cosine_sim_matrix(a: np.ndarray, b: np.ndarray) -> np.ndarray:
    """a: (N, D), b: (D,) â†’ (N,) ì½”ì‚¬ì¸ ìœ ì‚¬ë„"""
    a_norm = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-8)
    b_norm = b / (np.linalg.norm(b) + 1e-8)
    return np.dot(a_norm, b_norm)


# ==========================
# 3) ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ==========================
if "docs" not in st.session_state:
    st.session_state.docs = []  # ê° ìš”ì†Œ: dict(id, name, text, chunks, embeddings, overview, keywords, toc, highlights)

if "compare_result" not in st.session_state:
    st.session_state.compare_result = None

if "chat_histories" not in st.session_state:
    # ë¬¸ì„œë³„ ì±„íŒ… íˆìŠ¤í† ë¦¬: {doc_id: [ {role: "user"/"assistant", content: "..."} ]}
    st.session_state.chat_histories = {}


# ==========================
# 1) í™”ë©´ ì œëª© & ëª¨ë“œ ì•ˆë‚´
# ==========================
st.set_page_config(page_title="Synapse Docs", page_icon="ğŸ“˜", layout="wide")

st.title("ğŸ“˜ Synapse Docs")
st.caption("AI ë¬¸ì„œ ìš”ì•½ & ë§¥ë½ ë¶„ì„ ë¹„ì„œ (PDF / TXT ì§€ì›)")

if FALLBACK_REASON == "no_openai_key":
    st.toast("OPENAI_API_KEYê°€ ì—†ì–´ ìë™ìœ¼ë¡œ Ollama ëª¨ë“œë¡œ ì „í™˜í–ˆìŠµë‹ˆë‹¤.", icon="âš™ï¸")
else:
    st.toast(f"í˜„ì¬ LLM ì œê³µì: {EFFECTIVE_PROVIDER}", icon="ğŸ¤–")


# ==========================
# 4) ì‚¬ì´ë“œë°” â€“ ë¬¸ì„œ ì—…ë¡œë“œ & ì„ íƒ
# ==========================
with st.sidebar:
    st.header("ğŸ“‚ ë¬¸ì„œ ì—…ë¡œë“œ")

    uploaded_files = st.file_uploader(
        "PDF / TXT íŒŒì¼ ì—¬ëŸ¬ ê°œ ì—…ë¡œë“œ",
        type=["pdf", "txt"],
        accept_multiple_files=True,
    )

    if st.button("ë¬¸ì„œ ë¶„ì„ ì‹œì‘", use_container_width=True) and uploaded_files:
        for f in uploaded_files:
            with st.spinner(f"â–¶ {f.name} ë¶„ì„ ì¤‘..."):
                raw_text = extract_text_from_file(f)
                if not raw_text.strip():
                    st.warning(f"{f.name}: í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    continue

                chunks = chunk_text(raw_text, max_chars=1200)
                embeddings = llm.embed(chunks)

                # ê°œìš” ìš”ì•½ & í‚¤ì›Œë“œ ì¶”ì¶œ (ê¸´ ë¬¸ì„œë©´ ì•ë¶€ë¶„ë§Œ ìš”ì•½ì— ì‚¬ìš©)
                base_for_summary = raw_text[:8000]
                overview = llm.summarize(base_for_summary, mode="overview", level="ê¸°ë³¸")
                keywords = llm.summarize(base_for_summary, mode="keywords")
                toc = llm.summarize(base_for_summary, mode="toc")
                highlights = llm.summarize(base_for_summary, mode="highlights")

                doc_id = len(st.session_state.docs)
                st.session_state.docs.append(
                    {
                        "id": doc_id,
                        "name": f.name,
                        "text": raw_text,
                        "chunks": chunks,
                        "embeddings": embeddings,
                        "overview": overview,
                        "keywords": keywords,
                        "toc": toc,
                        "highlights": highlights,
                    }
                )
        st.success("ë¬¸ì„œ ë¶„ì„ ì™„ë£Œ!")

    st.markdown("---")
    st.header("ğŸ“‘ ë¬¸ì„œ ì„ íƒ")

    if st.session_state.docs:
        doc_names = [d["name"] for d in st.session_state.docs]
        current_doc_name = st.selectbox(
            "ë¶„ì„í•  ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš”",
            options=doc_names,
            index=0,
            key="selected_doc_name",
        )
    else:
        current_doc_name = None


# ==========================
# 5) ë©”ì¸ ì˜ì—­ â€“ ì„ íƒ ë¬¸ì„œ ìƒì„¸
# ==========================
if not st.session_state.docs:
    st.info("ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  'ë¬¸ì„œ ë¶„ì„ ì‹œì‘'ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
else:
    # ì„ íƒëœ ë¬¸ì„œ ì°¾ê¸°
    selected_idx = next(
        i for i, d in enumerate(st.session_state.docs) if d["name"] == current_doc_name
    )
    selected_doc = st.session_state.docs[selected_idx]

    st.subheader(f"ğŸ“„ ì„ íƒëœ ë¬¸ì„œ: {selected_doc['name']}")

    col1, col2 = st.columns([2, 1])

    # ===== ìš”ì•½ + ìš”ì•½ ìˆ˜ì¤€ ì„ íƒ =====
    with col1:
        st.markdown("#### ğŸ“Œ ë¬¸ì„œ ê°œìš” ìš”ì•½")

        # ìš”ì•½ ìˆ˜ì¤€ ì„ íƒ
        summary_level = st.radio(
            "ìš”ì•½ ìˆ˜ì¤€ ì„ íƒ",
            ["ì§§ê²Œ", "ê¸°ë³¸", "ìì„¸íˆ"],
            horizontal=True,
            key=f"summary_level_{selected_doc['id']}",
        )

        if st.button("ì´ ìˆ˜ì¤€ìœ¼ë¡œ ë‹¤ì‹œ ìš”ì•½", key=f"resummarize_{selected_doc['id']}"):
            with st.spinner("ì„ íƒí•œ ìˆ˜ì¤€ìœ¼ë¡œ ë‹¤ì‹œ ìš”ì•½ ì¤‘..."):
                base_for_summary = selected_doc["text"][:8000]
                new_overview = llm.summarize(
                    base_for_summary, mode="overview", level=summary_level
                )
                # ì„¸ì…˜ ìƒíƒœì˜ ë¬¸ì„œ ì •ë³´ ì—…ë°ì´íŠ¸
                st.session_state.docs[selected_idx]["overview"] = new_overview
                selected_doc["overview"] = new_overview
                st.success("ìš”ì•½ì„ ìƒˆë¡œ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

        st.write(selected_doc["overview"])

    # ===== í‚¤ì›Œë“œ + ìš”ì•½ ë‹¤ìš´ë¡œë“œ =====
    with col2:
        st.markdown("#### ğŸ·ï¸ í‚¤ì›Œë“œ / íƒœê·¸")
        st.write(selected_doc["keywords"])

        # ìš”ì•½ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
        download_text = (
            f"ë¬¸ì„œëª…: {selected_doc['name']}\n\n"
            f"[ìš”ì•½]\n{selected_doc['overview']}\n\n"
            f"[í‚¤ì›Œë“œ]\n{selected_doc['keywords']}\n"
        )
        st.download_button(
            "ğŸ’¾ í˜„ì¬ ìš”ì•½ì„ TXTë¡œ ì €ì¥",
            data=download_text,
            file_name=f"{selected_doc['name']}_summary.txt",
            mime="text/plain",
            key=f"download_{selected_doc['id']}",
        )

    st.markdown("---")
    # ìë™ ìƒì„± ëª©ì°¨
    with st.expander("ğŸ“š ìë™ ìƒì„± ëª©ì°¨", expanded=False):
        toc_text = selected_doc.get("toc", "").strip()
        if toc_text:
            st.markdown(toc_text)
        else:
            st.write("ëª©ì°¨ ì •ë³´ê°€ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # ì¤‘ìš” ë¬¸ì¥ í•˜ì´ë¼ì´íŠ¸
    with st.expander("âœ¨ ì¤‘ìš” ë¬¸ì¥ í•˜ì´ë¼ì´íŠ¸", expanded=False):
        hl_text = selected_doc.get("highlights", "").strip()
        if hl_text:
            st.markdown(hl_text)
        else:
            st.write("ì¤‘ìš” ë¬¸ì¥ í•˜ì´ë¼ì´íŠ¸ê°€ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # ì›ë¬¸ ë¯¸ë¦¬ë³´ê¸°
    with st.expander("ğŸ“ ì›ë¬¸ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ì•ë¶€ë¶„)", expanded=False):
        st.text(selected_doc["text"][:4000])

    # ===== ì—¬ëŸ¬ ë¬¸ì„œ ë¹„êµ / í†µí•© ìš”ì•½ =====
    if len(st.session_state.docs) >= 2:
        st.markdown("---")
        st.markdown("### ğŸ“Š ì—¬ëŸ¬ ë¬¸ì„œ ë¹„êµ / í†µí•© ìš”ì•½")

        doc_names = [d["name"] for d in st.session_state.docs]
        default_compare = [selected_doc["name"]]

        selected_for_compare = st.multiselect(
            "ë¹„êµí•  ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš” (ìµœì†Œ 2ê°œ)",
            options=doc_names,
            default=default_compare,
            key="compare_select",
        )

        if (
            len(selected_for_compare) >= 2
            and st.button("ğŸ” ì„ íƒí•œ ë¬¸ì„œ ë¹„êµ ìš”ì•½ ìƒì„±", key="compare_button")
        ):
            with st.spinner("ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ë¹„êµ ë¶„ì„ ì¤‘..."):
                blocks = []
                for d in st.session_state.docs:
                    if d["name"] in selected_for_compare:
                        blocks.append(
                            f"[ë¬¸ì„œëª…]: {d['name']}\n"
                            f"[ìš”ì•½]: {d['overview']}\n"
                            f"[í‚¤ì›Œë“œ]: {d['keywords']}\n"
                            f"[ë³¸ë¬¸ ì•ë¶€ë¶„]: {d['text'][:1500]}\n"
                        )
                compare_input = "\n\n====================\n\n".join(blocks)

                if EFFECTIVE_PROVIDER == "openai":
                    system_prompt = (
                        "ì—¬ëŸ¬ ê°œì˜ í•œêµ­ì–´ ë¬¸ì„œë¥¼ ë¹„êµ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
                        "ê° ë¬¸ì„œì˜ ê³µí†µì , ì°¨ì´ì , íŠ¹ì§•ì ì¸ ë¶€ë¶„ì„ ì •ë¦¬í•´ ì£¼ì„¸ìš”."
                    )
                    user_prompt = (
                        "ì•„ë˜ì— ì—¬ëŸ¬ ë¬¸ì„œì˜ ìš”ì•½Â·í‚¤ì›Œë“œÂ·ë³¸ë¬¸ ì¼ë¶€ê°€ ì •ë¦¬ë˜ì–´ ìˆë‹¤.\n"
                        "- ê° ë¬¸ì„œì˜ ê³µí†µì ê³¼ ì°¨ì´ì ì„ í•­ëª©ë³„ë¡œ ì •ë¦¬í•´ì¤˜.\n"
                        "- ì–´ë–¤ ë¬¸ì„œê°€ ì–´ë–¤ ê´€ì /ì£¼ì œë¥¼ ë” ê°•ì¡°í•˜ëŠ”ì§€ë„ ì„¤ëª…í•´ì¤˜.\n"
                        "- ë§ˆì§€ë§‰ì—ëŠ” ì¢…í•©ì ì¸ ê²°ë¡ ì„ 3~5ì¤„ë¡œ ì •ë¦¬í•´ì¤˜.\n\n"
                        f"{compare_input}"
                    )
                    resp = llm.client.responses.create(
                        model="gpt-4.1-mini",
                        input=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt},
                        ],
                    )
                    st.session_state.compare_result = resp.output_text
                else:
                    system_prompt = (
                        "ë‹¹ì‹ ì€ ì—¬ëŸ¬ ê°œì˜ í•œêµ­ì–´ ë¬¸ì„œë¥¼ ë¹„êµ ë¶„ì„í•˜ëŠ” ì˜¤í”„ë¼ì¸ ë¹„ì„œì…ë‹ˆë‹¤. "
                        "ê³µí†µì ê³¼ ì°¨ì´ì ì„ êµ¬ì¡°ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”."
                    )
                    user_content = (
                        "ì•„ë˜ì— ì—¬ëŸ¬ ë¬¸ì„œì˜ ìš”ì•½Â·í‚¤ì›Œë“œÂ·ë³¸ë¬¸ ì¼ë¶€ê°€ ì •ë¦¬ë˜ì–´ ìˆë‹¤.\n"
                        "- ê° ë¬¸ì„œì˜ ê³µí†µì ê³¼ ì°¨ì´ì ì„ í•­ëª©ë³„ë¡œ ì •ë¦¬í•´ì¤˜.\n"
                        "- ì–´ë–¤ ë¬¸ì„œê°€ ì–´ë–¤ ê´€ì /ì£¼ì œë¥¼ ë” ê°•ì¡°í•˜ëŠ”ì§€ë„ ì„¤ëª…í•´ì¤˜.\n"
                        "- ë§ˆì§€ë§‰ì—ëŠ” ì¢…í•©ì ì¸ ê²°ë¡ ì„ 3~5ì¤„ë¡œ ì •ë¦¬í•´ì¤˜.\n\n"
                        f"{compare_input}"
                    )
                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_content},
                    ]
                    response = ollama.chat(model=OLLAMA_MODEL, messages=messages)
                    if hasattr(response, "message"):
                        msg = response.message
                        if isinstance(msg, dict):
                            st.session_state.compare_result = msg.get("content", "")
                        else:
                            st.session_state.compare_result = msg.content
                    else:
                        st.session_state.compare_result = "ë¹„êµ ê²°ê³¼ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        if st.session_state.compare_result:
            st.markdown("#### ë¹„êµ ìš”ì•½ ê²°ê³¼")
            st.write(st.session_state.compare_result)

    # ===== Q&A ì„¹ì…˜ =====
    st.markdown("---")
    st.header("ğŸ’¬ ë¬¸ì„œ ê¸°ë°˜ Q&A")

    # ì´ ë¬¸ì„œì— ëŒ€í•œ ê°œë³„ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
    doc_id = selected_doc["id"]
    chat_history = st.session_state.chat_histories.setdefault(doc_id, [])

    # ê¸°ì¡´ ëŒ€í™” ì¶œë ¥
    for msg in chat_history:
        with st.chat_message("user" if msg["role"] == "user" else "assistant"):
            st.write(msg["content"])

    # ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
    user_question = st.chat_input(
        "ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ë‹µë³€í•˜ì§€ ì•Šë„ë¡ ì„¤ì •ë¨)"
    )

    if user_question:
        # 1) íˆìŠ¤í† ë¦¬ì— ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ê°€ & í™”ë©´ í‘œì‹œ
        chat_history.append({"role": "user", "content": user_question})
        with st.chat_message("user"):
            st.write(user_question)

        # 2) ì„ë² ë”© ê¸°ë°˜ ê´€ë ¨ ì¡°ê° ì°¾ê¸°
        question_vec = llm.embed([user_question])[0]
        sims = cosine_sim_matrix(selected_doc["embeddings"], question_vec)
        top_k = 5
        top_idx = np.argsort(-sims)[:top_k]
        context_blocks = [selected_doc["chunks"][i] for i in top_idx]

        context_text = "\n\n---\n\n".join(context_blocks)

        # 3) ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ â€“ ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ë¬´ì¡°ê±´ "ì—†ë‹¤"ë¡œ ë‹µí•˜ê²Œ
        system_instruction = (
            "ë„ˆëŠ” ë¡œì»¬ì—ì„œ ì‹¤í–‰ë˜ëŠ” í•œêµ­ì–´ ì „ìš© ì˜¤í”„ë¼ì¸ ë¬¸ì„œ ë¹„ì„œì•¼. "
            "ë°˜ë“œì‹œ ì•„ë˜ ê·œì¹™ì„ ì§€ì¼œì•¼ í•œë‹¤.\n"
            "1) ë‹µë³€ì— í¬í•¨ë˜ëŠ” ëª¨ë“  ì‚¬ì‹¤, ì¸ë¬¼, ê¸°ê´€, ë‚ ì§œ, ìˆ˜ì¹˜ëŠ” "
            "[ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸] ì•ˆì— ì‹¤ì œë¡œ ë“±ì¥í•˜ëŠ” ë‚´ìš©ì—ì„œë§Œ ê°€ì ¸ì™€ë¼.\n"
            "2) [ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸]ì— ë“±ì¥í•˜ì§€ ì•ŠëŠ” ì¸ë¬¼Â·ê¸°ê´€Â·ì‚¬ì‹¤ì— ëŒ€í•´ "
            "ì‚¬ìš©ìê°€ ì§ˆë¬¸í•˜ë©´ ë°˜ë“œì‹œ ì•„ë˜ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µí•´ë¼:\n"
            "   - 'ì´ ë¬¸ì„œì—ëŠ” ê·¸ ì¸ë¬¼(ì •ë³´)ì— ëŒ€í•œ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.'\n"
            "   - 'ì´ ë¬¸ì„œì—ì„œëŠ” í•´ë‹¹ ì£¼ì œë‚˜ ì¸ë¬¼ì´ ì–¸ê¸‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'\n"
            "3) ì ˆëŒ€ ìƒì‹, ì¶”ì¸¡, í•™ìŠµëœ ì§€ì‹ì„ ì´ìš©í•´ ë‚´ìš©ì„ ë³´ì™„í•˜ê±°ë‚˜ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆë¼.\n"
            "4) ë¬¸ì„œì— ìˆëŠ” ë‚´ìš©ë§Œ ê·¼ê±°ë¡œ ìš”ì•½í•˜ê±°ë‚˜ ì¸ìš©í•˜ë¼.\n"
            "5) ë‹µë³€ì€ í•­ìƒ í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•˜ê³ , ì˜ì–´Â·ì¤‘êµ­ì–´ ë“± ë‹¤ë¥¸ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆë¼.\n"
            "6) ë¬¸ì„œì— ê·¼ê±° ì—†ëŠ” ë‚´ìš©ìœ¼ë¡œ ëŒ€ë‹µí•˜ë ¤ê³  ì‹œë„í•  ê²½ìš°, ì¦‰ì‹œ ì¤‘ë‹¨í•˜ê³  ìœ„ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•´ë¼."
        )

        # 4) LLM í˜¸ì¶œ
        if EFFECTIVE_PROVIDER == "openai":
            resp = llm.client.responses.create(
                model="gpt-4.1-mini",
                input=[
                    {"role": "system", "content": system_instruction},
                    {
                        "role": "user",
                        "content": (
                            "ë‹¤ìŒì€ ì´ ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ ì¼ë¶€ ì»¨í…ìŠ¤íŠ¸ì´ë‹¤.\n\n"
                            f"[ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸]\n{context_text}\n\n"
                            f"[ì‚¬ìš©ì ì§ˆë¬¸]\n{user_question}"
                        ),
                    },
                ],
            )
            answer = resp.output_text
        else:
            messages = [
                {"role": "system", "content": system_instruction},
                {
                    "role": "user",
                    "content": (
                        "ë‹¤ìŒì€ ì´ ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ ì¼ë¶€ ì»¨í…ìŠ¤íŠ¸ì´ë‹¤.\n\n"
                        f"[ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸]\n{context_text}\n\n"
                        f"[ì‚¬ìš©ì ì§ˆë¬¸]\n{user_question}"
                    ),
                },
            ]
            response = ollama.chat(model=OLLAMA_MODEL, messages=messages)
            if hasattr(response, "message"):
                msg = response.message
                if isinstance(msg, dict):
                    answer = msg.get("content", "")
                else:
                    answer = msg.content
            else:
                answer = "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        # 5) ë‹µë³€ì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€ + í™”ë©´ì— í‘œì‹œ
        chat_history.append({"role": "assistant", "content": answer})
        with st.chat_message("assistant"):
            st.write(answer)

        # 6) ì´ë²ˆ ë‹µë³€ì—ì„œ ì°¸ê³ í•œ ë¬¸ì„œ ì¡°ê° ë³´ì—¬ì£¼ê¸°
        with st.expander("ğŸ“ ì´ë²ˆ ë‹µë³€ì—ì„œ ì°¸ê³ í•œ ë¬¸ì„œ ì¡°ê° ë³´ê¸°", expanded=False):
            for i, idx in enumerate(top_idx, start=1):
                st.markdown(f"**ì¡°ê° {i} (ìœ ì‚¬ë„: {sims[idx]:.3f})**")
                st.text(selected_doc["chunks"][idx])
                st.markdown("---")

